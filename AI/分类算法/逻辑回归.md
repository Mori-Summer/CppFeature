逻辑回归（Logistic Regression）是一种广泛应用于二分类问题的**统计学习方法**，尽管名称中带有“回归”，但它本质上是一种**分类算法**。

它通过引入 **Sigmoid 函数**（或称为 Logit 函数）将线性回归的输出值映射到 $0$ 到 $1$ 之间，从而表示事件发生的**概率**。

---

## 1. 核心思想与应用场景

### 核心思想

逻辑回归的目的是找到一组特征权重 $\mathbf{w}$ 和偏置 $b$，使得通过线性组合和 Sigmoid 函数转换后的结果，能够**最好地拟合**训练数据中事件发生的概率。

$$P(Y=1 \mid \mathbf{x}) = \sigma(\mathbf{w}^T \mathbf{x} + b)$$

| **变量 / 符号**                       | **意义**             | **详细解释**                                                                                                                         |                                                                   |
| --------------------------------- | ------------------ | -------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------- |
| **$P(Y=1 \mid \mathbf{x})$**      | **条件概率（模型输出）**     | 表示在给定输入特征向量 $\mathbf{x}$ 的条件下，目标变量 $Y$ 取值为 $1$（即事件发生或属于正类别）的**概率**。这个值就是逻辑回归模型的最终预测结果，范围在 $[0, 1]$ 之间。                           |                                                                   |
| **$\sigma(\cdot)$**               | **Sigmoid 函数**     | 是一个非线性激活函数，它将任意实数映射到 $(0, 1)$ 的区间，从而将线性得分转化为概率。它的定义是：                                                                            | $$\sigma(z) = \frac{1}{1 + e^{-z}}$$                              |
| **$\mathbf{w}$**                  | **权重向量** (Weights) | 模型的参数之一，是一个向量。$\mathbf{w} = (w_1, w_2, \dots, w_n)$。每个分量 $w_j$ 表示对应特征 $x_j$ 对最终概率的**重要性**和**影响方向**。$w_j$ 越大（绝对值），特征 $x_j$ 的影响越大。 |                                                                   |
| **$\mathbf{x}$**                  | **输入特征向量**         | 输入到模型的特征数据。$\mathbf{x} = (x_1, x_2, \dots, x_n)^T$。例如，在预测是否患病时，$\mathbf{x}$ 可能包含年龄、血压、体重等特征值。                                    |                                                                   |
| **$b$**                           | **偏置项** (Bias)     | 模型的参数之一，是一个标量。它独立于输入特征，用于调整决策边界的位置，可以理解为**基准概率**的对数转换。                                                                           |                                                                   |
| **$\mathbf{w}^T \mathbf{x}$**     | **内积（线性组合）**       | 是 $\mathbf{w}$ 和 $\mathbf{x}$ 的点积。它计算了输入特征的**加权和**：                                                                              | $$\mathbf{w}^T \mathbf{x} = w_1 x_1 + w_2 x_2 + \dots + w_n x_n$$ |
| **$\mathbf{w}^T \mathbf{x} + b$** | **线性得分（Logit）**    | 这是逻辑回归的**线性部分**，也被称为 **Logit**。这个值是 Sigmoid 函数的输入，范围在 $(-\infty, +\infty)$。它决定了最终概率的“强度”和方向。                                     |                                                                   |
### 适用场景

逻辑回归主要用于二分类问题，例如：

- **医疗诊断：** 根据病人体征判断患病（1）或未患病（0）的概率。
    
- **金融风控：** 判断客户是否会违约（1）或不会违约（0）。
    
- **市场营销：** 预测用户是否会点击广告（1）或不会点击（0）。
    

---

## 2. 算法的数学模型

逻辑回归模型分为两个主要部分：**线性回归部分** 和 **Sigmoid 激活部分**。

### A. 线性回归部分 (Linear Model)

首先，模型计算输入特征 $\mathbf{x}$ 的线性组合：

$$z = \mathbf{w}^T \mathbf{x} + b$$

其中：

- $z$：线性得分（Logit）。
    
- $\mathbf{w} = (w_1, w_2, \dots, w_n)$：权重向量。
    
- $\mathbf{x} = (x_1, x_2, \dots, x_n)^T$：输入特征向量。
    
- $b$：偏置项（Bias）。
    

### B. Sigmoid 函数（Logit 逆函数）

Sigmoid 函数（记作 $\sigma(z)$）将任意实数 $z$ 映射到 $(0, 1)$ 区间内。

$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

#### Sigmoid 函数的特性：

- 当 $z \to +\infty$ 时，$\sigma(z) \to 1$。
    
- 当 $z \to -\infty$ 时，$\sigma(z) \to 0$。
    
- 当 $z = 0$ 时，$\sigma(z) = 0.5$。
    

### C. 概率输出 (Final Output)

将线性得分 $z$ 代入 Sigmoid 函数，得到输出 $h_{\mathbf{w}, b}(\mathbf{x})$，表示 $Y=1$ 的概率：

$$h_{\mathbf{w}, b}(\mathbf{x}) = P(Y=1 \mid \mathbf{x}) = \frac{1}{1 + e^{-(\mathbf{w}^T \mathbf{x} + b)}}$$

**决策边界：** 当 $h_{\mathbf{w}, b}(\mathbf{x}) \ge 0.5$ 时，预测类别为 $1$；否则为 $0$。这等价于判断 $\mathbf{w}^T \mathbf{x} + b \ge 0$。

---

## 3. 模型训练：损失函数与优化

模型训练的目标是找到最佳的 $\mathbf{w}$ 和 $b$，使得预测概率与训练数据的真实标签最接近。

### A. 损失函数：交叉熵（Cross-Entropy / Log Loss）

由于逻辑回归的输出是概率，不能直接使用线性回归的均方误差（MSE）。逻辑回归采用**最大似然估计**，其等价于最小化**交叉熵损失**（也称为对数损失，Log Loss）。

对于单个样本 $( \mathbf{x}^{(i)}, y^{(i)} )$：

$$\text{Loss}(h_{\mathbf{w}, b}(\mathbf{x}^{(i)}), y^{(i)}) = - [ y^{(i)} \log(h^{(i)}) + (1 - y^{(i)}) \log(1 - h^{(i)}) ]$$

其中 $h^{(i)} = h_{\mathbf{w}, b}(\mathbf{x}^{(i)})$.

- 如果真实标签 $y^{(i)}=1$，则只保留第一项：$\text{Loss} = -\log(h^{(i)})$。此时，如果预测概率 $h^{(i)}$ 越接近 $1$，损失越小。
    
- 如果真实标签 $y^{(i)}=0$，则只保留第二项：$\text{Loss} = -\log(1 - h^{(i)})$。此时，如果预测概率 $h^{(i)}$ 越接近 $0$，损失越小。
    

### B. 优化方法：梯度下降（Gradient Descent）

总成本函数 $J(\mathbf{w}, b)$ 是所有样本损失的平均值（或和）。

$$J(\mathbf{w}, b) = - \frac{1}{m} \sum_{i=1}^{m} [ y^{(i)} \log(h^{(i)}) + (1 - y^{(i)}) \log(1 - h^{(i)}) ]$$

我们通过**梯度下降**算法来最小化 $J(\mathbf{w}, b)$：

1. **计算梯度：** 对每个参数 $\theta \in \{\mathbf{w}, b\}$ 计算偏导数 $\frac{\partial J}{\partial \theta}$。
    
2. **更新参数：** 按照梯度的反方向以学习率 $\alpha$ 更新参数。
    

$$\theta := \theta - \alpha \frac{\partial J(\mathbf{w}, b)}{\partial \theta}$$

梯度计算结果（令人惊讶地与线性回归的形式相似）：

$$\frac{\partial J(\mathbf{w}, b)}{\partial w_j} = \frac{1}{m} \sum_{i=1}^{m} (h^{(i)} - y^{(i)}) x_j^{(i)}$$

---

## 4. 与线性回归的区别

|**特性**|**线性回归 (Linear Regression)**|**逻辑回归 (Logistic Regression)**|
|---|---|---|
|**任务类型**|回归（预测连续值）|分类（预测离散类别/概率）|
|**输出范围**|$(-\infty, +\infty)$|$(0, 1)$ (概率)|
|**激活函数**|无（恒等函数）|**Sigmoid 函数**|
|**损失函数**|均方误差 (MSE)|**交叉熵损失 / 对数损失**|
|**决策边界**|不存在（只有拟合线）|存在（线性决策边界 $\mathbf{w}^T \mathbf{x} + b = 0$）|

---

## 5. 优势与局限性

### 优势

- **计算效率高：** 训练和预测速度快。
    
- **可解释性强：** 模型的权重 $\mathbf{w}$ 可以直接解释特征对结果概率的影响程度和方向（例如，权重为正表示特征值越大，事件发生的概率越大）。
    
- **易于实现：** 基于强大的数学理论，容易实现和调试。
    

### 局限性

- **必须是线性可分：** 逻辑回归的决策边界是线性的。对于非线性可分的复杂数据集，性能会很差。
    
- **特征工程要求高：** 需要人工进行大量的特征构造和转换（如多项式特征、交叉特征）来处理非线性问题。
    
- **不适合处理海量特征：** 在特征维度非常高时（如深度学习领域），其性能和效果不如更复杂的模型。